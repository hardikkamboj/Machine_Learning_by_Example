{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score,classification_report,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a non spam email\n",
    "file_path_ham = r'E:\\programming\\dataset\\email_spam_detection\\ham\\0007.1999-12-14.farmer.ham.txt'\n",
    "sample_ham = open('sample_ham.txt','w')\n",
    "with open(file_path_ham,'r') as infile:\n",
    "    sample_ham.write(infile.read())\n",
    "sample_ham.close()    \n",
    "\n",
    "\n",
    "file_path_spam = r'E:\\programming\\dataset\\email_spam_detection\\spam\\0058.2003-12-21.GP.spam.txt'\n",
    "sample_spam = open('samepl_spam.txt','w')\n",
    "with open(file_path_spam,'r') as infile:\n",
    "    sample_spam.write(infile.read())\n",
    "sample_spam.close()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(path,index,emails,labels):\n",
    "    \"\"\"\"\n",
    "    arguements - \n",
    "        path - the dir of the folder containting the text files for emails\n",
    "        index - the label for the particular folder\n",
    "                (in this its 0 for non spam, 1 for spam)\n",
    "        emails - list where emails will be stored\n",
    "        labels - list where the labels will be stored\n",
    "    Return  - \n",
    "        emails - the updated list of emails\n",
    "        labels - the updated list of labels\n",
    "    \"\"\"\n",
    "    for file in glob.glob(os.path.join(path,'*.txt')):\n",
    "        with open(file,'r',encoding = 'ISO-8859-1') as infile:\n",
    "            emails.append(infile.read())\n",
    "        labels.append(index)\n",
    "    return emails,labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ham = r'E:\\programming\\dataset\\email_spam_detection\\ham'\n",
    "path_spam = r'E:\\programming\\dataset\\email_spam_detection\\spam'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "emails = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails,labels = load_emails(path_spam,1,emails,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails,labels = load_emails(path_ham,0,emails,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n",
      "5172\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(emails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the raw data\n",
    "#### 1.number and punctuation removal\n",
    "#### 2.human names removal\n",
    "#### 3.stop words removal\n",
    "#### 4.lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number and punc removal\n",
    "def letters_only(astr):\n",
    "    return astr.isalpha()\n",
    "\n",
    "#names \n",
    "all_names = set(names.words())\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(docs):\n",
    "    \"\"\"\n",
    "    arguement - \n",
    "       docs - collecetion (list) of documents containing text\n",
    "       \n",
    "    return - \n",
    "       cleaned_doc - lemmanatizing, removing symbols,punctuations, numbers, and human names from the input\n",
    "    \"\"\"\n",
    "    cleaned_doc = []\n",
    "    for doc in docs:\n",
    "        cleaned_doc.append(\" \".join(lemmatizer.lemmatize(word)\n",
    "                          for word in doc.split()\n",
    "                          if letters_only(word) and \n",
    "                          not word in all_names))\n",
    "    return cleaned_doc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_emails = clean_doc(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coca cola mbna america nascar partner with otcbb imts stock profile about company investment highlight press release indianapolis in race car simulator ink the sale of eight simulator for installation in moscow indianapolis in nascar silicon motor speedway simulator go international indianapolis in nascar silicon motor speedway expands to monterey california s famed cannery row indianapolis in nascar silicon motor speedway announces custom upgrade to world s most realistic racing simulation indianapolis in race car simulator and baldacci sign agreement to develop international market for the new generation race simulutors indianapolis in imts form new subsidiary for manufacturing and sale of race car simulator indianapolis in nascar silicon motor speedway renews licensing agreement with speedway motorsports inc for race track simulator indianapolis in nascar silicon motor speedway int speedway corp renew licensing agreement for race track simulator indianapolis in nascar silicon motor speedway simulator to be installed at st louis nascar speedpark location indianapolis in nascar silicon motor speedway operator get exclusive five year nascar license extension nashville tn nascar silicon motor speedway at opry mill to host official medium luncheon for nashville superspeedway s trace adkins chrome event indianapolis in nascar silicon motor speedway simulator now running at nascar speedpark indianapolis in nascar silicon motor speedway expansion plan begin at two burroughs chapin entertainment venue indianapolis in nascar silicon motor speedway to determine national champion among simulator racer indianapolis in partnership with coca cola mbna and in demand boost nascar silicon motor speedway racing center indianapolis in nascar driver sadler nadeau give thumb up to indianapolis simulation at nascar silicon motor speedway indianapolis in star studded lineup for make a wish fundraiser at nashville nascar silicon motor speedway location indianapolis in indianapolis motor speedway to be added to nascar silicon motor speedway simulator important notice and disclaimer please read intelligent stock pick and affiliate isp publishes report providing information on selected company that isp belief ha investment potential isp is not a registered investment advisor or broker dealer this report is provided a an information service only and the statement and opinion in this report should not be construed a an offer or solicitation to buy or sell any security isp accepts no liability for any loss arising from an investor s reliance on or use of this report an investment in imts is considered to be highly speculative and should not be considered unless a person can afford a complete loss of investment isp ha agreed to profile imts in conjunction with a obligation that one of isp s affiliate owes to a third party sbr for the publication and circulation of this report isp owns no share in imts stock at or about the time of publication of this report subsequently isp may buy or sell share of imts stock in the open market this report contains forward looking statement which involve risk and uncertainty that may cause actual result to differ materially from those set forth in the forward looking statement for further detail concerning these risk and uncertainty see the sec filing of imts including the company s most recent annual and quarterly report to stop receiving these email send a blank email to unsub ppkkqpkgimzpx upper web side com\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_emails[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: coca cola , mbna america , nascar partner with otcbb : imts\\nstock\\nprofile\\nabout\\ncompany\\ninvestment\\nhighlights\\npress release\\n12 / 01 / 2003\\nindianapolis , in - race car simulators ? inks the sale of eight simulators for installation in moscow\\n09 / 17 / 2003\\nindianapolis , in - nascar silicon motor speedway ? simulators go international\\n09 / 05 / 2003\\nindianapolis , in - nascar silicon motor speedway ? expands to monterey , california ' s famed cannery row\\n09 / 02 / 2003\\nindianapolis , in - nascar silicon motor speedway ? announces custom upgrades to world ' s most realistic racing simulation\\n08 / 14 / 2003\\nindianapolis , in - race car simulators ? and baldacci sign agreement to develop international markets for the new generation race simulutors\\n08 / 12 / 2003\\nindianapolis , in - imts forms new subsidiary for manufacturing and sales of race car simulators\\n08 / 07 / 2003\\nindianapolis , in - nascar silicon motor speedway ? renews licensing agreement with speedway motorsports , inc . , for race track simulators\\n08 / 05 / 2003\\nindianapolis , in - nascar silicon motor speedway ? , int . speedway corp . renew licensing agreement for race track simulators\\n07 / 27 / 2003\\nindianapolis , in - nascar silicon motor speedway ? simulators to be installed at st . louis nascar speedpark location\\n07 / 24 / 2003\\nindianapolis , in - nascar silicon motor speedway ? operator gets exclusive five - year nascar license extension\\n05 / 30 / 2003\\nnashville , tn - nascar silicon motor speedway ? at opry mills to host official media luncheon for nashville superspeedway ' s trace adkins chrome 300 event\\n04 / 22 / 2003\\nindianapolis , in - nascar silicon motor speedway ? simulators now running at nascar speedpark\\n03 / 19 / 2003\\nindianapolis , in - nascar silicon motor speedway ? expansion plans begin at two burroughs chapin entertainment venues\\n02 / 27 / 2003\\nindianapolis , in - nascar silicon motor speedway ? to determine national champion among simulator racers\\n02 / 14 / 2003\\nindianapolis , in - partnerships with coca - cola , mbna and\\nin demand boost nascar silicon motor speedway ? racing centers\\n02 / 28 / 2003\\nindianapolis , in - nascar drivers sadler , nadeau give thumbs up to indianapolis simulation at nascar silicon motor speedway ?\\n02 / 22 / 2003\\nindianapolis , in - star studded lineup for make a wish fundraiser at nashville nascar silicon motor speedway location\\n01 / 14 / 2003\\nindianapolis , in - indianapolis motor speedway to be added to nascar silicon motor speedway simulators\\n* * * * * * * important notice and\\ndisclaimer : please read * * * * * * *\\nintelligent stock picks , and affiliates ( isp ) , publishes reports providing information on selected companies that isp believes has investment potential . isp is not a registered investment advisor or broker - dealer . this report is provided as an information service only , and the statements and opinions in this report should not be construed as an offer or solicitation to buy or sell any security . isp accepts no liability for any loss arising from an investor ' s reliance on or use of this report . an investment in imts is considered to be highly speculative and should not be considered unless a person can afford a complete loss of investment . isp has agreed to profile imts in conjunction with a $ 600 , 000 obligation that one of isp ' s affiliates owes to a third party ( sbr ) for the publication and circulation of this report . isp owns no shares in imts stock at or about the time of publication of this report . subsequently isp may buy or sell shares of imts stock in the open market . this report contains forward - looking statements , which involve risks , and uncertainties that may cause actual results to differ materially from those set forth in the forward - looking statements . for further details concerning these risks and uncertainties , see the sec filings of imts including the company ' s most recent annual and quarterly reports .\\nto stop receiving these emails , send a blank email to unsub - ppkkqpkgimzpx @ upper - web - side . com\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the cleaned emails\n",
    "#it also removes the stop words\n",
    "cv = CountVectorizer(stop_words='english',max_features=500)\n",
    "term_docs = cv.fit_transform(cleaned_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5172, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 70)\t1\n",
      "  (0, 480)\t1\n",
      "  (0, 121)\t2\n",
      "  (0, 415)\t1\n",
      "  (0, 204)\t1\n",
      "  (0, 370)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 375)\t2\n",
      "  (0, 249)\t2\n",
      "  (0, 155)\t2\n",
      "  (0, 311)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 322)\t1\n",
      "  (0, 465)\t1\n",
      "  (0, 388)\t2\n",
      "  (0, 45)\t2\n",
      "  (0, 297)\t1\n",
      "  (0, 411)\t3\n",
      "  (0, 393)\t1\n",
      "  (0, 350)\t1\n",
      "  (0, 209)\t2\n",
      "  (0, 367)\t8\n",
      "  (0, 293)\t1\n",
      "  (0, 254)\t1\n",
      "  (0, 57)\t1\n",
      "  :\t:\n",
      "  (0, 496)\t1\n",
      "  (0, 245)\t2\n",
      "  (0, 408)\t1\n",
      "  (0, 83)\t1\n",
      "  (0, 259)\t2\n",
      "  (0, 490)\t1\n",
      "  (0, 210)\t2\n",
      "  (0, 379)\t2\n",
      "  (0, 364)\t1\n",
      "  (0, 73)\t3\n",
      "  (0, 414)\t4\n",
      "  (0, 23)\t1\n",
      "  (0, 387)\t1\n",
      "  (0, 154)\t1\n",
      "  (0, 395)\t2\n",
      "  (0, 389)\t1\n",
      "  (0, 394)\t1\n",
      "  (0, 17)\t3\n",
      "  (0, 212)\t5\n",
      "  (0, 310)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 285)\t2\n",
      "  (0, 250)\t2\n",
      "  (0, 445)\t1\n",
      "  (0, 178)\t2\n"
     ]
    }
   ],
   "source": [
    "print(term_docs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'access', 'account', 'accounting', 'act', 'action', 'activity', 'actual', 'actuals', 'add', 'additional', 'address', 'adobe', 'advice', 'advise', 'aep', 'agree', 'agreement', 'aimee', 'align', 'allen', 'allocated', 'allocation', 'america', 'ami', 'anita', 'aol', 'application', 'april', 'area', 'attached', 'august', 'availability', 'available', 'based', 'believe', 'best', 'better', 'bob', 'book', 'border', 'br', 'brenda', 'brian', 'business', 'buy', 'buyback', 'called', 'calpine', 'camp', 'canada', 'carlos', 'case', 'cc', 'cd', 'ce', 'cec', 'center', 'change', 'changed', 'charge', 'check', 'chokshi', 'cialis', 'city', 'clem', 'click', 'clynes', 'coastal', 'color', 'com', 'come', 'communication', 'company', 'complete', 'computron', 'confirm', 'contact', 'content', 'continue', 'contract', 'control', 'copy', 'corp', 'corporation', 'correct', 'cost', 'cotten', 'counterparty', 'cover', 'create', 'created', 'current', 'currently', 'customer', 'daily', 'daren', 'darren', 'data', 'date', 'david', 'day', 'deal', 'dec', 'december', 'delivery', 'demand', 'desk', 'development', 'did', 'direct', 'doc', 'doe', 'dollar', 'don', 'donna', 'duke', 'eastrans', 'ect', 'ee', 'effective', 'email', 'employee', 'ena', 'end', 'energy', 'enron', 'enronxgate', 'entered', 'entex', 'equistar', 'error', 'estimate', 'event', 'exchange', 'face', 'facility', 'fact', 'family', 'farmer', 'fax', 'feb', 'february', 'fee', 'feel', 'field', 'file', 'financial', 'firm', 'flow', 'flowed', 'follow', 'following', 'font', 'form', 'forward', 'forwarded', 'fred', 'free', 'friday', 'fuel', 'future', 'fw', 'fyi', 'gary', 'gas', 'gathering', 'gc', 'gco', 'george', 'getting', 'global', 'going', 'good', 'got', 'graf', 'great', 'group', 'ha', 'hank', 'height', 'help', 'hi', 'high', 'home', 'hope', 'hou', 'hour', 'houston', 'howard', 'hpl', 'hplc', 'hplno', 'hplo', 'href', 'hsc', 'html', 'http', 'id', 'iferc', 'im', 'image', 'imbalance', 'include', 'including', 'increase', 'index', 'industry', 'info', 'information', 'international', 'internet', 'investment', 'invoice', 'issue', 'item', 'iv', 'jackie', 'james', 'jan', 'january', 'john', 'jones', 'julie', 'july', 'june', 'just', 'katherine', 'katy', 'know', 'lannou', 'le', 'lee', 'let', 'letter', 'life', 'like', 'limited', 'line', 'link', 'lisa', 'list', 'little', 'll', 'lloyd', 'location', 'logistics', 'long', 'look', 'looking', 'loss', 'love', 'low', 'mail', 'make', 'management', 'manager', 'march', 'mark', 'market', 'marketing', 'mary', 'meeting', 'megan', 'melissa', 'message', 'meter', 'meyers', 'michael', 'microsoft', 'midcon', 'mike', 'million', 'mmbtu', 'monday', 'money', 'month', 'morning', 'na', 'natural', 'nbsp', 'nd', 'need', 'needed', 'net', 'new', 'news', 'newsletter', 'nom', 'nomination', 'noms', 'north', 'note', 'notice', 'november', 'number', 'october', 'offer', 'office', 'oil', 'old', 'online', 'operating', 'operation', 'opportunity', 'option', 'order', 'original', 'page', 'paid', 'partner', 'party', 'past', 'pat', 'path', 'pay', 'payment', 'pec', 'pefs', 'people', 'performance', 'period', 'person', 'pg', 'phone', 'pill', 'pipe', 'pipeline', 'place', 'plan', 'plant', 'pm', 'point', 'pop', 'position', 'possible', 'power', 'prescription', 'price', 'pricing', 'prior', 'problem', 'process', 'producer', 'product', 'production', 'professional', 'program', 'project', 'provide', 'provided', 'pt', 'purchase', 'quality', 'question', 'rate', 'ray', 'read', 'real', 'receive', 'received', 'record', 'regard', 'regarding', 'release', 'remove', 'reply', 'report', 'request', 'resource', 'result', 'review', 'revised', 'revision', 'right', 'risk', 'rita', 'robert', 'said', 'sale', 'save', 'say', 'schedule', 'scheduled', 'scheduling', 'scott', 'section', 'security', 'sell', 'send', 'sent', 'september', 'server', 'service', 'set', 'share', 'sherlyn', 'sitara', 'site', 'size', 'smith', 'software', 'soon', 'source', 'special', 'spot', 'spreadsheet', 'src', 'st', 'start', 'state', 'statement', 'stephanie', 'steve', 'stock', 'stop', 'storage', 'subject', 'suite', 'supply', 'support', 'sure', 'survey', 'susan', 'swing', 'tap', 'tax', 'taylor', 'td', 'te', 'team', 'technology', 'teco', 'tenaska', 'term', 'texas', 'th', 'thank', 'thanks', 'thing', 'think', 'thought', 'thu', 'thursday', 'ticket', 'time', 'today', 'tom', 'tomorrow', 'total', 'tr', 'trade', 'trading', 'transaction', 'transport', 'try', 'tu', 'tuesday', 'tx', 'txu', 'type', 'unify', 'unit', 'united', 'update', 'use', 'used', 'user', 'using', 'utility', 'valid', 'valley', 'vance', 've', 'viagra', 'visit', 'volume', 'wa', 'want', 'way', 'web', 'website', 'wednesday', 'week', 'weissman', 'wellhead', 'width', 'window', 'work', 'working', 'world', 'www', 'wynne', 'xl', 'xp', 'yahoo', 'year', 'young', 'zero', 'zone']\n"
     ]
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': 125, 'ha': 178, 'called': 47, 'young': 497, 'le': 231, 'time': 445, 'production': 345, 'loss': 250, 'swing': 424, 'new': 285, 'color': 69, 'read': 357, 'website': 481, 'prescription': 337, 'low': 252, 'cost': 86, 'online': 301, 'order': 306, 'direct': 110, 'click': 66, 'thanks': 438, 'list': 241, 'people': 319, 'change': 58, 'able': 0, 'partner': 310, 'team': 430, 'investment': 212, 'account': 2, 'agreement': 17, 'project': 348, 'based': 34, 'contact': 77, 'set': 394, 'business': 44, 'request': 368, 'act': 4, 'opportunity': 304, 'come': 71, 'send': 389, 'million': 272, 'united': 463, 'state': 410, 'dollar': 113, 'area': 29, 'cover': 89, 'regard': 362, 'better': 37, 'special': 404, 'following': 152, 'tax': 426, 'share': 395, 'provide': 349, 'process': 342, 'money': 275, 'long': 247, 'form': 154, 'need': 282, 'security': 387, 'john': 221, 'number': 295, 'america': 23, 'stock': 414, 'company': 73, 'release': 364, 'sale': 379, 'international': 210, 'world': 490, 'market': 259, 'corp': 83, 'st': 408, 'location': 245, 'year': 496, 'event': 133, 'plan': 329, 'demand': 106, 'center': 57, 'make': 254, 'notice': 293, 'report': 367, 'information': 209, 'provided': 350, 'service': 393, 'statement': 411, 'offer': 297, 'buy': 45, 'sell': 388, 'use': 465, 'person': 322, 'complete': 74, 'party': 311, 'forward': 155, 'looking': 249, 'risk': 375, 'actual': 7, 'result': 370, 'including': 204, 'stop': 415, 'email': 121, 'web': 480, 'com': 70, 'additional': 10, 'work': 488, 'technology': 431, 'today': 446, 'doc': 111, 'wa': 477, 'like': 236, 'want': 478, 'th': 436, 'size': 399, 'day': 101, 'charge': 60, 'xl': 493, 'good': 173, 'morning': 277, 'free': 158, 'best': 36, 'mail': 253, 'real': 358, 'product': 344, 'office': 298, 'right': 374, 'hour': 187, 'pay': 315, 'available': 33, 'currently': 93, 'item': 215, 'doe': 112, 'pill': 325, 'help': 181, 'visit': 475, 'site': 398, 'let': 233, 'limited': 237, 'add': 9, 'sure': 421, 'purchase': 352, 'check': 61, 'week': 483, 'getting': 170, 'using': 468, 've': 473, 'viagra': 474, 'price': 338, 'place': 328, 'http': 197, 'www': 491, 'link': 239, 'html': 196, 'january': 220, 'thank': 437, 'software': 401, 'try': 455, 'start': 409, 'action': 5, 'cd': 54, 'don': 114, 'receive': 359, 'end': 124, 'gary': 164, 'used': 466, 'deal': 102, 'record': 361, 'march': 257, 'access': 1, 'month': 276, 'customer': 94, 'save': 380, 'professional': 346, 'valid': 470, 'internet': 211, 'info': 208, 'payment': 316, 'remove': 365, 'case': 52, 'reply': 366, 'message': 265, 'operation': 303, 'ce': 55, 'effective': 120, 'source': 403, 'content': 78, 'type': 460, 'window': 487, 'microsoft': 269, 'image': 201, 'nd': 281, 'align': 19, 'border': 40, 'tr': 450, 'td': 428, 'src': 407, 'width': 486, 'height': 180, 'believe': 35, 'just': 226, 'look': 248, 'href': 194, 'index': 206, 'nbsp': 280, 'font': 153, 'family': 138, 'pt': 351, 'going': 172, 'love': 251, 'face': 135, 'newsletter': 287, 'address': 11, 'br': 41, 'david': 100, 'desk': 107, 'note': 292, 'include': 203, 'phone': 324, 'great': 176, 'news': 286, 'activity': 6, 'll': 243, 'home': 184, 'line': 238, 'hope': 185, 'know': 229, 'date': 99, 'book': 39, 'thing': 439, 'term': 434, 'prior': 340, 'canada': 50, 'transport': 454, 'possible': 335, 'original': 307, 'fax': 140, 'xp': 494, 'way': 479, 'letter': 234, 'advise': 14, 'daily': 95, 'feel': 144, 'cialis': 63, 'current': 92, 'high': 183, 'city': 64, 'create': 90, 'page': 308, 'north': 291, 'dec': 103, 'june': 225, 'entered': 128, 'mike': 271, 'corporation': 84, 'fee': 143, 'development': 108, 'rate': 355, 'received': 360, 'steve': 413, 'past': 312, 'option': 305, 'unit': 462, 'field': 145, 'regarding': 363, 'data': 98, 'global': 171, 'future': 161, 'net': 284, 'brian': 43, 'oil': 299, 'power': 336, 'friday': 159, 'fact': 137, 'robert': 377, 'pm': 331, 'exchange': 134, 'program': 347, 'control': 81, 'hi': 182, 'did': 109, 'quality': 353, 'got': 174, 'position': 334, 'adobe': 12, 'suite': 418, 'update': 464, 'sent': 390, 'confirm': 76, 'jan': 219, 'life': 235, 'increase': 205, 'question': 354, 'delivery': 105, 'communication': 72, 'error': 131, 'farmer': 139, 'firm': 148, 'problem': 341, 'point': 332, 'transaction': 453, 'advice': 13, 'tuesday': 457, 'mark': 258, 'resource': 369, 'follow': 151, 'say': 381, 'thought': 441, 'estimate': 132, 'contract': 80, 'management': 255, 'support': 420, 'facility': 136, 'industry': 207, 'section': 386, 'supply': 419, 'fw': 162, 'survey': 422, 'review': 371, 'december': 104, 'financial': 147, 'changed': 59, 'little': 242, 'manager': 256, 'needed': 283, 'user': 467, 'paid': 309, 'trading': 452, 'id': 198, 'producer': 343, 'said': 378, 'gas': 165, 'issue': 214, 'schedule': 382, 'performance': 320, 'group': 177, 'revised': 372, 'operating': 302, 'thursday': 443, 'trade': 451, 'wednesday': 482, 'cc': 53, 'server': 392, 'feb': 141, 'natural': 279, 'file': 146, 'copy': 82, 'subject': 417, 'flow': 149, 'february': 142, 'think': 440, 'hank': 179, 'september': 391, 'old': 300, 'path': 314, 'application': 27, 'availability': 32, 'ect': 118, 'mary': 261, 'august': 31, 'ray': 356, 'george': 169, 'working': 489, 'utility': 469, 'brenda': 42, 'stephanie': 412, 'zero': 498, 'houston': 188, 'nom': 288, 'te': 429, 'total': 449, 'texas': 435, 'james': 218, 'agree': 16, 'pg': 323, 'jones': 222, 'zone': 499, 'soon': 402, 'tomorrow': 448, 'duke': 116, 'attached': 30, 'volume': 476, 'employee': 122, 'pop': 333, 'july': 224, 'created': 91, 'scheduled': 383, 'pipeline': 327, 'spot': 405, 'correct': 85, 'tx': 458, 'aol': 26, 'na': 278, 'plant': 330, 'meeting': 262, 'monday': 274, 'computron': 75, 'marketing': 260, 'continue': 79, 'ticket': 444, 'april': 28, 'ee': 119, 'allocation': 22, 'meyers': 267, 'scott': 385, 'bob': 38, 'iv': 216, 'thu': 442, 'storage': 416, 'october': 296, 'meter': 266, 'logistics': 246, 'yahoo': 495, 'donna': 115, 'period': 321, 'smith': 400, 'camp': 49, 'graf': 175, 'pipe': 326, 'fuel': 160, 'accounting': 3, 'allen': 20, 'ami': 24, 'pricing': 339, 'taylor': 427, 'jackie': 217, 'coastal': 68, 'katherine': 227, 'fred': 157, 'november': 294, 'michael': 268, 'invoice': 213, 'megan': 263, 'unify': 461, 'lee': 232, 'imbalance': 202, 'julie': 223, 'howard': 189, 'lloyd': 244, 'im': 200, 'gc': 167, 'tom': 447, 'gathering': 166, 'vance': 472, 'forwarded': 156, 'carlos': 51, 'tap': 425, 'hou': 186, 'valley': 471, 'lisa': 240, 'nomination': 289, 'susan': 423, 'weissman': 484, 'daren': 96, 'melissa': 264, 'enron': 126, 'darren': 97, 'calpine': 48, 'fyi': 163, 'sherlyn': 396, 'anita': 25, 'allocated': 21, 'hpl': 190, 'teco': 432, 'buyback': 46, 'clem': 65, 'counterparty': 88, 'sitara': 397, 'hplc': 191, 'entex': 129, 'rita': 376, 'wynne': 492, 'chokshi': 62, 'scheduling': 384, 'mmbtu': 273, 'aimee': 18, 'lannou': 230, 'midcon': 270, 'noms': 290, 'wellhead': 485, 'hsc': 195, 'revision': 373, 'equistar': 130, 'pat': 313, 'txu': 459, 'actuals': 8, 'clynes': 67, 'gco': 168, 'ena': 123, 'spreadsheet': 406, 'iferc': 199, 'katy': 228, 'flowed': 150, 'eastrans': 117, 'cec': 56, 'pec': 317, 'pefs': 318, 'tu': 456, 'hplo': 193, 'cotten': 87, 'tenaska': 433, 'aep': 15, 'hplno': 192, 'enronxgate': 127}\n"
     ]
    }
   ],
   "source": [
    "feature_mapping = cv.vocabulary_ \n",
    "print(feature_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_index(labels,indices):\n",
    "    \"\"\"\n",
    "    Arguement - \n",
    "        labels - THe list of labels for all the documents\n",
    "        indices - The list of different labels used\n",
    "    Returns - \n",
    "        ans - dict of the form {index:[--indices where label == index]--}\n",
    "    \"\"\"\n",
    "    ans = {index:[] for index in indices}\n",
    "    for i in range(len(labels)):\n",
    "        ans[labels[i]].append(i)\n",
    "    return ans    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171]\n"
     ]
    }
   ],
   "source": [
    "label_index = get_label_index(labels,[0,1])\n",
    "print(label_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_label_index(labels):\n",
    "#     from collections import defaultdict\n",
    "#     label_index = defaultdict(list)\n",
    "#     for index, label in enumerate(labels):\n",
    "#         label_index[label].append(index)\n",
    "#     return label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_label_index(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the labels index above we will calulate the prior probability\n",
    "def get_prior(label_index):\n",
    "    \"\"\"\n",
    "    Arguments - \n",
    "      label_index - dict (output from func get_label_index)\n",
    "    Return - \n",
    "       prior - dict - {index:--prior_prob--}\n",
    "    \"\"\"\n",
    "    prior = {index:len(item) for index,item in label_index.items()}\n",
    "    total = sum(prior.values())\n",
    "    for key in prior:\n",
    "        prior[key] = prior[key] / total\n",
    "    return prior    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7099767981438515, 1: 0.2900232018561485}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = (get_prior(label_index))\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(term_document_matrix,label_index,smoothing = 0):\n",
    "    \"\"\"\n",
    "    Arguements - \n",
    "        term_document_matrix - sparse_matrix (num_ex,num_features)\n",
    "        label_index - dict - {label - list of indices}\n",
    "        smoothing - int - addiditve laplase smoothing parameter\n",
    "    Returns - \n",
    "       likelihood - dict - (class - prob of feature given class)\n",
    "          P(x[i] / y == j) = likelihood[j][i]\n",
    "                  \n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label,index in label_index.items():\n",
    "        likelihood[label] = term_document_matrix[index,:].sum(axis = 0) + smoothing\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothing = 1\n",
    "likelihood = get_likelihood(term_docs,label_index,smoothing = 1)\n",
    "len(likelihood[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00108581, 0.00095774, 0.00087978, 0.00084637, 0.00010023])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00108997, 0.00141902, 0.00456555, 0.0005347 , 0.00421594])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood[1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'access', 'account', 'accounting', 'act']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    \"\"\" Compute posterior of testing samples, based on prior and likelihood\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        prior (dictionary, with class label as key, corresponding prior as the value)\n",
    "        likelihood (dictionary, with class label as key, corresponding conditional probability vector as value)\n",
    "    Returns:\n",
    "        dictionary, with class label as key, corresponding posterior as value\n",
    "    \"\"\"\n",
    "    num_docs = term_document_matrix.shape[0]\n",
    "    posteriors = []\n",
    "    for i in range(num_docs):\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        # = exp(log(prior * likelihood))\n",
    "        # = exp(log(prior) + log(likelihood))\n",
    "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(i)\n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "        # exp(-1000):exp(-999) will cause zero division error,\n",
    "        # however it equates to exp(0):exp(1)\n",
    "        min_log_posterior = min(posterior.values())\n",
    "        for label in posterior:\n",
    "            try:\n",
    "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "            except:\n",
    "                # if one's log value is excessively large, assign it infinity\n",
    "                posterior[label] = float('inf')\n",
    "        # normalize so that all sums up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emails = []\n",
    "with open('sample_spam.txt','r',encoding = 'ISO-8859-1') as infile:\n",
    "            test_emails.append(infile.read())\n",
    "with open('sample_ham.txt','r',encoding = 'ISO-8859-1') as infile:\n",
    "            test_emails.append(infile.read())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = clean_doc(test_emails)\n",
    "test_docs = cv.transform(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 500)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = get_posterior(test_docs,prior=prior,likelihood=likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.00011308226374159875, 1: 0.9998869177362584},\n",
       " {0: 1.0, 1: 7.872408281950292e-27}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implimenting from sklearn\n",
    "#alpha is smoothing parameter\n",
    "nb = MultinomialNB(1.0,fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(term_docs,labels,test_size = 0.33,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3465, 500)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1707, 500)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test,test_classes)\n",
    "cm = confusion_matrix(y_test,test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is 0.9214997070884593\n"
     ]
    }
   ],
   "source": [
    "print('the accuracy is {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the confustion matrix is \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1100,   91],\n",
       "       [  43,  473]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the confustion matrix is ')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TN,FP),(FN,TP) = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235936188077246"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision - fraction of positive calls that are correct\n",
    "precision = TN/(TN + FP)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall - fraction of true positivs that are correctly identified\n",
    "recall = TP / (TP + FN)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1_score - harmonice mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test,test_classes)\n",
    "recall = recall_score(y_test,test_classes)\n",
    "f1 = f1_score(y_test,test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.8386524822695035\n",
      "Recall is 0.9166666666666666\n",
      "F1 score is 0.8759259259259258\n"
     ]
    }
   ],
   "source": [
    "print('Precision is {}'.format(precision))\n",
    "print('Recall is {}'.format(recall))\n",
    "print('F1 score is {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1191\n",
      "           1       0.84      0.92      0.88       516\n",
      "\n",
      "    accuracy                           0.92      1707\n",
      "   macro avg       0.90      0.92      0.91      1707\n",
      "weighted avg       0.92      0.92      0.92      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we can also get the above mentioned results in one command\n",
    "report = classification_report(y_test,test_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "k_fold = StratifiedKFold(n_splits = k)\n",
    "\n",
    "#creating numpy arrays for better slicing\n",
    "cleaned_emails_np = np.array(cleaned_emails)\n",
    "labels_np = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_option = [2000, 4000, 8000]\n",
    "smoothing_factor_option = [0.5, 1.0, 1.5, 2.0]\n",
    "fit_prior_option = [True, False]\n",
    "auc_record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_indices,test_indices in k_fold.split(cleaned_emails_np,labels_np):\n",
    "    X_train,X_test = cleaned_emails_np[train_indices],cleaned_emails_np[test_indices]\n",
    "    y_train,y_test = labels_np[train_indices],labels_np[test_indices]\n",
    "    for features in max_features_option:\n",
    "        if not features in auc_record:\n",
    "            auc_record[features] = {}\n",
    "        cv = CountVectorizer(stop_words = 'english',max_features = features)\n",
    "        train_doc = cv.fit_transform(X_train)\n",
    "        test_doc = cv.transform(X_test)\n",
    "        for smoothing in smoothing_factor_option:\n",
    "            if not smoothing in auc_record[features]:\n",
    "                auc_record[features][smoothing] = {}\n",
    "            for fit_prior in fit_prior_option:\n",
    "                clf = MultinomialNB(alpha=smoothing, fit_prior=fit_prior)\n",
    "                clf.fit(train_doc,y_train)\n",
    "                pred_probas = clf.predict_proba(test_doc)\n",
    "                pos_prob = pred_probas[:,1]\n",
    "                auc = roc_auc_score(y_test,pos_prob)  \n",
    "                auc_record[features][smoothing][fit_prior] \\\n",
    "                    = auc + auc_record[features][smoothing].get(fit_prior, 0.0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(auc_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000: {0.5: {True: 9.744341507720254, False: 9.743687186549776}, 1.0: {True: 9.726073579354736, False: 9.725047017533468}, 1.5: {True: 9.7146733206966, False: 9.715017869130829}, 2.0: {True: 9.706112180626308, False: 9.706747324566601}}, 4000: {0.5: {True: 9.81694519310508, False: 9.814603892706236}, 1.0: {True: 9.796673651423607, False: 9.797172678987483}, 1.5: {True: 9.785206778422777, False: 9.786758875330728}, 2.0: {True: 9.778234090550091, False: 9.77867877028788}}, 8000: {0.5: {True: 9.85627517474233, False: 9.854758174386921}, 1.0: {True: 9.845380632231569, False: 9.845271097421316}, 1.5: {True: 9.840752033724282, False: 9.841142390317103}, 2.0: {True: 9.837345101291318, False: 9.837871672985033}}}\n"
     ]
    }
   ],
   "source": [
    "print(auc_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_features   smoothing    fit_prior      auc\n",
      "2000           0.5           True        0.9744\n",
      "2000           0.5           False        0.9744\n",
      "2000           1.0           True        0.9726\n",
      "2000           1.0           False        0.9725\n",
      "2000           1.5           True        0.9715\n",
      "2000           1.5           False        0.9715\n",
      "2000           2.0           True        0.9706\n",
      "2000           2.0           False        0.9707\n",
      "4000           0.5           True        0.9817\n",
      "4000           0.5           False        0.9815\n",
      "4000           1.0           True        0.9797\n",
      "4000           1.0           False        0.9797\n",
      "4000           1.5           True        0.9785\n",
      "4000           1.5           False        0.9787\n",
      "4000           2.0           True        0.9778\n",
      "4000           2.0           False        0.9779\n",
      "8000           0.5           True        0.9856\n",
      "8000           0.5           False        0.9855\n",
      "8000           1.0           True        0.9845\n",
      "8000           1.0           False        0.9845\n",
      "8000           1.5           True        0.9841\n",
      "8000           1.5           False        0.9841\n",
      "8000           2.0           True        0.9837\n",
      "8000           2.0           False        0.9838\n"
     ]
    }
   ],
   "source": [
    "print('Max_features   smoothing    fit_prior      auc')\n",
    "for max_features,max_features_data in auc_record.items():\n",
    "    for smoothing,smoothing_data in max_features_data.items():\n",
    "        for fit_prior,auc in smoothing_data.items():\n",
    "            print(\"{0}           {1}           {2}        {3:.4f}\".format(max_features,smoothing,fit_prior,auc/k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best selection is \n",
    "best_features = 8000\n",
    "best_smoothing = 0.5\n",
    "best_fit_prior = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
