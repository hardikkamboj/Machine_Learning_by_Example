{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    return 1.0 / (1 + np.exp(-input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction(X,weights):\n",
    "    \"\"\"\n",
    "    ARGS - \n",
    "      X - ndarray - features\n",
    "      W - ndarray - weights\n",
    "    RETURNS - \n",
    "     Y - ndarray - predictions\n",
    "     \"\"\"\n",
    "    return sigmoid(np.dot(X,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_gd(X_train,y_train,weights,learning_rate):\n",
    "    \"\"\"\n",
    "    ARGS - \n",
    "      X_trian,y_trian - ndarray - (features,labels)\n",
    "      weights - ndarray\n",
    "      learning_rate -float\n",
    "    Returns - \n",
    "      weights - updated weights\n",
    "    \"\"\"\n",
    "    predictions = compute_prediction(X_train,weights)\n",
    "    weights_delta = np.dot(X_train.T,y_train - predictions)\n",
    "    weights += learning_rate / float(y_train.shape[0]) * weights_delta\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X,y,weights):\n",
    "    \"\"\"\n",
    "    ARGS - \n",
    "      X,y - ndarray - (features,labels)\n",
    "      weights - ndarray\n",
    "    RETURNS - \n",
    "      cost - float - log_loss\n",
    "    \"\"\"\n",
    "    predictions = compute_prediction(X,weights)\n",
    "    cost = np.mean( -y*np.log(predictions) - (1-y)*np.log(1 - predictions))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train,y_train,max_itr,learning_rate,fit_intercept = False):\n",
    "    \"\"\"\n",
    "    runs the logistic regression\n",
    "    ARGS - \n",
    "      X_train,y_train - ndarray - (features,labels)\n",
    "      max_itr - int - maximum iterations\n",
    "      learning_rate - float\n",
    "      fit_intercept - boolean - whether or not to include constant weight\n",
    "    RETURN - \n",
    "      weights - ndarray - updated weights\n",
    "    \"\"\"\n",
    "    if fit_intercept:\n",
    "        intercept = np.ones(shape = (X_train.shape[0],1))\n",
    "        X_train = np.hstack((intercept,X_train))\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "    for i in range(max_itr):\n",
    "        weights = update_weights_gd(X_train,y_train,weights,learning_rate)\n",
    "        if i %100 == 0:\n",
    "            print(\"cost at {} iteration is {}\".format(i,compute_cost(X_train,y_train,weights)))\n",
    "    return weights        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,weights):\n",
    "    if X.shape[1] == weights.shape[0] - 1:\n",
    "        intercept = np.ones((X.shape[0],1))\n",
    "        X = np.hstack((intercept,X))\n",
    "    return compute_prediction(X,weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A example\n",
    "X_train = np.array([[6, 7],\n",
    "                    [2, 4],\n",
    "                    [3, 6],\n",
    "                    [4, 7],\n",
    "                    [1, 6],\n",
    "                    [5, 2],\n",
    "                    [2, 0],\n",
    "                    [6, 3],\n",
    "                    [4, 1],\n",
    "                    [7, 2]])\n",
    "\n",
    "y_train = np.array([0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    1,\n",
    "                    1,\n",
    "                    1,\n",
    "                    1,\n",
    "                    1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at 0 iteration is 0.5744042371657099\n",
      "cost at 100 iteration is 0.034460223392510314\n",
      "cost at 200 iteration is 0.018265572708476157\n",
      "cost at 300 iteration is 0.012493458388041109\n",
      "cost at 400 iteration is 0.00951532913854744\n",
      "cost at 500 iteration is 0.007693388060646923\n",
      "cost at 600 iteration is 0.0064620943335143306\n",
      "cost at 700 iteration is 0.0055735118468322594\n",
      "cost at 800 iteration is 0.004901632254525041\n",
      "cost at 900 iteration is 0.004375567740665121\n"
     ]
    }
   ],
   "source": [
    "weights = train_logistic_regression(X_train,y_train,max_itr=1000,learning_rate=0.1,fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[6, 1],\n",
    "                   [1, 3],\n",
    "                   [3, 1],\n",
    "                   [4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_test,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAXf0lEQVR4nO3dfXBU9b3H8feXkGQJohEJqKhQWypXHcrD1odKFUUUxYFba68ytdbHXLX2xrGtV2hvnTp1pq1Oi+MUpwhKrU+DUK2DQrX3ipWqwAIWlWitgBVEiSICeSZ87x+7aoAlhLBnz+4vn9dMJsnvLOf32Yl+cvLbs+eYuyMiIuHpEXcAERGJhgpeRCRQKngRkUCp4EVEAqWCFxEJVM+4A7TXr18/Hzx4cNwxRESKxvLlyz9096ps2wqq4AcPHkwqlYo7hohI0TCzd/a2TUs0IiKBUsGLiARKBS8iEigVvIhIoFTwIiKBiqzgzew4M3ul3cdWM7sxqvlEotDWBr/9LQwdCgMHwrXXwvvvx52qe3vpJRg3Dg4/HM44AxYtijtR4bJ8XE3SzEqADcDJ7r7XU3qSyaTrNEkpJFdcAXPmQEND+vvSUujXD1avhsrKeLN1R4sWwfnnQ2Pj52MVFemf0YQJscWKlZktd/dktm35WqIZC7zdUbmLFJp16+DRRz8vd4DWVtiyBWbOjC1Wt3bTTbuWO6R/PjdqbSCrfBX8JcAjeZpLJCdWrICysj3HGxvh+efzn0fgtdeyj7/9dno5TXYVecGbWRkwEXhsL9urzSxlZqm6urqo44h02jHHZC+N0lL40pfyn0egf//s45WVUFKS3yzFIB9H8OcBK9z9g2wb3X2GuyfdPVlVlfVyCiKxGDUKhgyBnrtd0KO0FG64IZ5M3d3Uqek19/YqKuCHP4wnT6HLR8FPRsszUoTM4JlnYOzY9FJNIgGDB8NTT8EXvxh3uu7puuvSJX/QQelir6hIr7/fckvcyQpTpGfRmFkF8C5wrLt/sq/H6ywaKVQff5x+Me/II9PFL/FqboYPPkgv2SQScaeJV0dn0UR6NUl3bwAOi3IOkXw49ND0hxSG8vL0ayTSMb2TVUQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAIVTMFv3Ag/+hGcfDJceimsXBl3ou5txw6YPRvGjIEzz4QHH4S2trhTiXQvkd5028wqgZnAiYADV7r7S7meZ906GDUKtm+HlhZIpeDxx2HOHJgwIdezyb64wze+Ac89B/X16bFly+BPf0r/TMzizSfSXUR9BH8XsNDdhwJfAWqjmOQnP4FPPkmXO8DOndDQANXV6bKR/HrhhV3LHdJfL1iQLnoRyY/ICt7MDgZOB2YBuHuLu2+JYq6//CX7n/+bN6eXbiS/nnsu/Qt2d83N6W0ikh9RHsEfC9QB95vZSjObaWa9d3+QmVWbWcrMUnV1dV2aqG/f7OM7d0KfPl3apRyAfv0gkdhzvLw8vU1E8iPKgu8JjATucfcRQD1wy+4PcvcZ7p5092RVVVWXJvrBD6D3br86ysth4kQVfBwuuQR6ZPkvq0cPuOii/OcR6a6iLPj1wHp3X5L5fi7pws+5K6+Ea69Nl/ohh6SPHr/+dZg1K4rZZF8OOwyeeip9tN6nT/qjf39YuDD98xGR/IjsLBp3f9/M3jWz49z9TWAssDqKuczgzjthyhR47TU45hj4wheimEk664wz0q9/LF+e/vmMGgUlJXGnEuleIj1NEvg+8JCZlQFrgCuinOyww9LFIoWhZ8/0+xJEJB6RFry7vwIko5xDRESyC+adrCIisisVvIhIoFTwIiKBUsGLiARKBS8iEigVvIhIoFTwIiKBUsGLiARKBS8iEigVvIhIoFTwIiKBUsGLiARKBS8iEigVvIhIoFTwIiKBUsGLiARKBS8iEigVvIhIoFTwIp2wZg08/njcKUT2T6QFb2brzOxVM3vFzFJRziUSpTvugGuugZ07404i0nmR3nQ740x3/zAP84jkVEMD3H57+vPDD0NTU7rkDzkELr8chg2LO6FIx/JR8CJFqawMXngh/fGp+++Hvn3hyivjyyXSWVGvwTvwjJktN7PqbA8ws2ozS5lZqq6uLuI4Ip3XsycsWgTnnff5WGkpvPkmnHhibLFEOi3qgj/N3UcC5wHfM7PTd3+Au89w96S7J6uqqiKOI7J/zGDFinTZ9+8PLS3wr3/FnUqkcyIteHd/L/N5E/A4cFKU84nk2o4dcOqpsHIlvP02VFfD9u1xpxLpnMjW4M2sN9DD3bdlvj4HuC2q+USiUFq66+mRv/tdfFlE9leUL7IOAB43s0/nedjdF0Y4n4iItBNZwbv7GuArUe1fREQ6pneyiogESgUvIhIoFbyISKBU8CIigVLBi4gESgUvIhIoFbyISKBU8CIigVLBi4gESgUvIhIoFbyISKBU8CIigVLBi4gESgUvIhIoFbyISKBU8CIigVLBi4gESgUvIhKoKO/JKl20YgU8+ywceihcdBH07Rt3IhEpRpEXvJmVAClgg7tfEPV8xcwdrrgCHnsMWlqgrAxuugmefBLOOivudCJSbPKxRFMD1OZhnqL3xBMwdy40NMCOHenP9fXwzW+mC19EZH9EWvBmdhQwAZgZ5TyhuP/+dKHvbudOWLw4/3lEpLhFfQQ/DbgZ2Lm3B5hZtZmlzCxVV1cXcZzC5t61bSIi2URW8GZ2AbDJ3Zd39Dh3n+HuSXdPVlVVRRWnKHz3u9C7d/Zto0fnN4uIFL8oj+BPAyaa2TrgUeAsM3swwvmK3oUXwgUXpEveDBIJqKiAOXOgvDzudCJSbMzz8Le/mY0Bfrivs2iSyaSnUqnI8xQyd3j55fRpkpWVcPHFMGBA3KlEpFCZ2XJ3T2bbpvPgC4wZnHpq+kNE5EDkpeDdfRGwKB9ziYhImi5VICISKBW8iEigVPAiIoFSwYuIBEoFLyISqA4L3swONrMvZhkfFl0kERHJhb0WvJn9B/AGMM/MXjezr7bbPDvqYCIicmA6OoKfCoxy9+HAFcAfzOzCzDaLPJmIiByQjt7oVOLuGwHcfamZnQnMz1wCWNc2FBEpcB0dwW9rv/6eKfsxwCTghIhziYjIAeqo4K8DepjZ8Z8OuPs2YDxwddTBRETkwOy14N397+7+FjDHzP7b0noBvwauz1tCERHpks6cB38ycDTwIrAMeI/0td5FRKSAdabgW4FGoBeQANa6+15vwSciIoWhMwW/jHTBfxUYDUw2s7mRphIRkQPWmevBX+Xun95m6X1gkpl9J8JMIiKSA/s8gm9X7u3H/hBNHBERyRVdbExEJFDBFfxrr8FOvQQsErRXX32VRx99lJUrV8YdpaBFdk9WM0sAfwXKM/PMdfdbo5oPYOtWGDUKnn4axo6NciYRiUNjYyMTJ07kxRdfpKSkhLa2NkaMGMGCBQvo06dP3PEKTpQ33W4GznL37WZWCiw2swXu/nKuJ3rnHaithSVLoKUF7roLWlthwAAYMSLXs4lIXKZOncrixYtpamr6bCyVSlFTU8N9990XY7LCZO7RXzfMzCqAxcB17r5kb49LJpOeSu3xmu4+3XYb3HorlJdDczP06gWNjXDGGbBoUddzi0hhqays5JNPPtljvLy8nMbGRsy634VuzWy5uyezbYt0Dd7MSszsFWAT8Gy2cjezajNLmVmqrq6uS/P89Kdwzz3w6e8qM/j2t+GZZw4gvIgUnPZH7u21trayUy++7SHSgnf3tsz15I8CTjKzE7M8Zoa7J909WVVV1eW5+vdPL8v06JE+ej/oICgrO4DwIlJwzj77bHr02LW2zIzTTjuNkpKSmFIVrrycRePuW4BFpK9EGYklS2D0aFi3Dm64AVasiGomEYnLXXfdRWVlJYlEAoBEIsHBBx/MPffcE3OywhTZGryZVQGt7r4lcxXKZ4Bfuvv8vf2brq7Bi0j38eGHH3LvvfeSSqUYPnw41dXVDBgwIO5YseloDT7Ks2iOAH5vZiWk/1KY01G5i4h0Rr9+/ZgyZUrcMYpCZAXv7qsAnaQoIhKT4N7JKiIiaSp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFupH169ezdOlStm3bFncUAerr61m6dCnvvPNOJPuPrODN7Ggze87Mas3sdTOriWouEenY1q1bGT9+PEOGDGHcuHEMGDCAn//853HH6tamTZtG//79GTduHEOHDmXMmDFs3rw5p3NEeQS/A/iBu/8bcArwPTM7PsL5RGQvLrvsMhYtWkRTUxNbt26lsbGRX/ziF8yZMyfuaN3SwoUL+fGPf0xDQwNbt26lqamJl156iW9961s5nSeygnf3je6+IvP1NqAWGBjVfCKS3ebNm1m4cCHNzc27jNfX1/OrX/0qplTd2x133EFDQ8MuYy0tLbz44ousX78+Z/PkZQ3ezAYDI4AlWbZVm1nKzFJ1dXX5iCPSrXz88cf07Nkz67ZNmzblOY0AbNy4Met4WVkZuezByAvezA4C5gE3uvvW3be7+wx3T7p7sqqqKuo4It3OoEGDSCQSe4yXlJRw9tlnx5BIxo8fT2lp6R7j7s7xx+duJTvSgjezUtLl/pC7/zHKuUQku549e3L33XdTUVHx2VhpaSmHHHIIt956a4zJuq+bb76ZQw89lLKyss/GKioquPPOOykvL8/ZPFGeRWPALKDW3X8d1Twism+TJ0/m2WefZdKkSQwbNozrr7+eVatWMWjQoLijdUuHH344q1atoqamhmHDhnH++eczf/58qqurczqPuXtOd/jZjs1GAy8ArwI7M8NT3f3pvf2bZDLpqVQqkjwiIiEys+Xunsy2LfsrLzng7osBi2r/IiLSMb2TVUQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXkQkUCp4EZFAqeAlUm1tbezcuXPfDyxw9fX1fPTRR0R1D2PZfzt27Ig7QsGLrODN7D4z22Rmr0U1hxS+mpoapkyZEneMLtu8eTOTJk2ib9++HHnkkQwdOpS//e1vccfq9rZs2cKRRx7Jxo0b445S0KI8gp8NjI9w/1LAmpubaWxs5KGHHmL27Nk0NTXR3Nwcd6z94u6ce+65LFiwgJaWFlpaWvjHP/7Bueeey9q1a+OO1y21tbXR3NzME088QV1dHfPmzaO5uZm2tra4oxWkyAre3f8KbI5q/1K4VqxYQSKRoKKigtbWVurr6+nVqxeJRILXX3897nidtnLlSmpra2ltbd1lvKWlhenTp8eUqnubPHkyiUSCq6++GoAbb7yRRCLBhAkTYk5WmGJfgzezajNLmVmqrq4u7jiSAyNHjuSBBx6goqKC+vp66uvr6d27N4888ggnnHBC3PE6bd26dZSUlOwx3trayhtvvBFDIpk+fTpjx44lkUgAkEgk+NrXvsbs2bPjDVagYi94d5/h7kl3T1ZVVcUdR3Lk0ksvpaKigrKyMkpLS6msrOTiiy+OO9Z+GT58OC0tLXuM9+rVi9GjR8eQSPr160d1dTVNTU307t2bpqYmrrrqKg4//PC4oxWk2AtewlRbW8tHH31ETU0N1113HRs2bCi6detjjz2WCy+8kIqKis/GSkpK6NOnD9dcc02Mybq3J554gv79+3PvvfcycOBA5s2bF3ekgmVRnvZlZoOB+e5+Ymcen0wmPZVKRZZH8qe5uZm1a9cydOhQAFavXs2QIUMoLS2NOdn+2bFjB7/5zW+YPn0627dvZ8KECdx+++0MHDgw7mjd1po1a6iqqqJPnz40NDSwYcMGhgwZEnes2JjZcndPZt0WVcGb2SPAGKAf8AFwq7vP6ujfqOBFRPZPRwXfM6pJ3X1yVPsWEZF90xq8iEigVPAiIoFSwYuIBEoFLyISKBW8iEigVPAiIoFSwYuIBEoFLyISKBW8iEigVPAiIoFSwYuIBEoFLyISKBW8iEigVPAiIoFSwYuIBEoFLyISKBW8iEigVPAiIoFSwYuIBCrSgjez8Wb2ppn908xuiXKuUNTW1nLOOeeQSCQ47LDDmDp1Ki0tLXHHEpEiFNlNt82sBPgtMA5YDywzsyfdfXVUcxa7DRs2cMopp7Bt2zbcnebmZqZNm8Zbb73FY489Fnc8ESkyUR7BnwT8093XuHsL8CgwKcL5it7dd99NU1MT7v7ZWGNjI/Pnz2ft2rUxJhORYhRlwQ8E3m33/frMmOzF0qVLsy7HlJeXs3q1/vARkf0TZcFbljHf40Fm1WaWMrNUXV1dhHEK3/DhwyktLd1jvKWlhS9/+csxJBKRYhZlwa8Hjm73/VHAe7s/yN1nuHvS3ZNVVVURxil8NTU1lJeX7zKWSCQYM2YMQ4YMiSmViBSrKAt+GTDEzL5gZmXAJcCTEc5X9AYNGsTzzz/PSSedhJnRq1cvLr/8cubOnRt3NBEpQpGdRePuO8zsBuDPQAlwn7u/HtV8oRg5ciRLliyhra2NHj16YJZtpUtEZN8iK3gAd38aeDrKOUJVUlISdwQRKXJ6J6uISKBU8CIigVLBi4gESgUvIhIoFbyISKCs/XVP4mZmdcA7OdhVP+DDHOwnbnoehUXPo/CE8lwO5HkMcves7xItqILPFTNLuXsy7hwHSs+jsOh5FJ5QnktUz0NLNCIigVLBi4gEKtSCnxF3gBzR8ygseh6FJ5TnEsnzCHINXkREwj2CFxHp9lTwIiKBCqrgzew+M9tkZq/FnaWrzOxoM3vOzGrN7HUzq4k7U1eZWcLMlprZ3zPP5WdxZ+oqMysxs5VmNj/uLAfCzNaZ2atm9oqZpeLO01VmVmlmc83sjcz/K6fGnWl/mdlxmZ/Dpx9bzezGnM4R0hq8mZ0ObAcecPcT487TFWZ2BHCEu68wsz7AcuDf3b3obspq6YvZ93b37WZWCiwGatz95Zij7TczuwlIAge7+wVx5+kqM1sHJN29qN8cZGa/B15w95mZGwpVuPuWuHN1lZmVABuAk909F2/2BAI7gnf3vwKb485xINx9o7uvyHy9DailSG9W7mnbM9+WZj6K7ojCzI4CJgAz484iYGYHA6cDswDcvaWYyz1jLPB2LssdAiv40JjZYGAEsCTeJF2XWdp4BdgEPOvuxfhcpgE3AzvjDpIDDjxjZsvNrDruMF10LFAH3J9ZNptpZr3jDnWALgEeyfVOVfAFyswOAuYBN7r71rjzdJW7t7n7cNI3XT/JzIpq6czMLgA2ufvyuLPkyGnuPhI4D/heZlmz2PQERgL3uPsIoB64Jd5IXZdZYpoIPJbrfavgC1BmvXoe8JC7/zHuPLmQ+RN6ETA+5ij76zRgYmbt+lHgLDN7MN5IXefu72U+bwIeB06KN1GXrAfWt/trcC7pwi9W5wEr3P2DXO9YBV9gMi9MzgJq3f3Xcec5EGZWZWaVma97AWcDb8Sbav+4+xR3P8rdB5P+M/r/3P3SmGN1iZn1zrxwT2ZJ4xyg6M44c/f3gXfN7LjM0Fig6E5CaGcyESzPQMQ33c43M3sEGAP0M7P1wK3uPiveVPvtNOA7wKuZtWuAqZkbmBebI4DfZ84Q6AHMcfeiPs2wyA0AHk8fQ9ATeNjdF8Ybqcu+DzyUWd5YA1wRc54uMbMKYBzwn5HsP6TTJEVE5HNaohERCZQKXkQkUCp4EZFAqeBFRAKlghcRCZQKXqQTzGyhmW0p9qtJSveighfpnDtIvz9BpGio4EXaMbOvmtmqzLXse2euY3+iu/8vsC3ufCL7I6h3soocKHdfZmZPAj8HegEPunvRvZ1fBFTwItncBiwDmoD/ijmLSJdpiUZkT32Bg4A+QCLmLCJdpoIX2dMM4H+Ah4BfxpxFpMu0RCPSjpldBuxw94czV8F80czOAn4GDAUOylyp9Cp3/3OcWUX2RVeTFBEJlJZoREQCpYIXEQmUCl5EJFAqeBGRQKngRUQCpYIXEQmUCl5EJFD/D/mdqIxpUUZBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train[:,0], X_train[:,1], c=['b']*5+['k']*5, marker='o')\n",
    "colours = ['k' if prediction >= 0.5 else 'b' for prediction in predictions]\n",
    "plt.scatter(X_test[:,0], X_test[:,1], marker='*', c=colours)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preicting click through rate with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ad_click_data(n, offset=0):\n",
    "    X_dict, y = [], []\n",
    "    with open(r'E:\\programming\\dataset\\click_throug_predictions\\train\\train.csv','r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for i in range(offset):\n",
    "            next(reader)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            i += 1\n",
    "            y.append(int(row['click']))\n",
    "            del row['click'], row['id'], row['hour'], row['device_id'], row['device_ip']\n",
    "            X_dict.append(row)\n",
    "            if i >= n:\n",
    "                break\n",
    "    return X_dict, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "X_dict_train, y_train = read_ad_click_data(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict_one_hot_encoder = DictVectorizer(sparse=False)\n",
    "X_train = dict_one_hot_encoder.fit_transform(X_dict_train)\n",
    "\n",
    "X_dict_test, y_test = read_ad_click_data(n, n)\n",
    "X_test = dict_one_hot_encoder.transform(X_dict_test)\n",
    "\n",
    "X_train_10k = X_train\n",
    "y_train_10k = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at 0 iteration is 0.6820019456743648\n",
      "cost at 100 iteration is 0.4608619713011896\n",
      "cost at 200 iteration is 0.4503715555130051\n",
      "cost at 300 iteration is 0.4455503890097847\n",
      "cost at 400 iteration is 0.4420611414384596\n",
      "cost at 500 iteration is 0.4393702812833892\n",
      "cost at 600 iteration is 0.437228041454526\n",
      "cost at 700 iteration is 0.4354781787758496\n",
      "cost at 800 iteration is 0.43401801289720104\n",
      "cost at 900 iteration is 0.4327779028622343\n",
      "cost at 1000 iteration is 0.4317091585700226\n",
      "cost at 1100 iteration is 0.43077673019057455\n",
      "cost at 1200 iteration is 0.4299546928842355\n",
      "cost at 1300 iteration is 0.42922339559221634\n",
      "cost at 1400 iteration is 0.4285676184571522\n",
      "cost at 1500 iteration is 0.42797535312823465\n",
      "cost at 1600 iteration is 0.4274369752561037\n",
      "cost at 1700 iteration is 0.42694466897530536\n",
      "cost at 1800 iteration is 0.42649201676958726\n",
      "cost at 1900 iteration is 0.42607370031421204\n",
      "cost at 2000 iteration is 0.42568527750493995\n",
      "cost at 2100 iteration is 0.42532301300292674\n",
      "cost at 2200 iteration is 0.4249837472238756\n",
      "cost at 2300 iteration is 0.4246647935395447\n",
      "cost at 2400 iteration is 0.4243638565943513\n",
      "cost at 2500 iteration is 0.4240789667070855\n",
      "cost at 2600 iteration is 0.42380842671759145\n",
      "cost at 2700 iteration is 0.42355076859163654\n",
      "cost at 2800 iteration is 0.42330471776471257\n",
      "cost at 2900 iteration is 0.42306916368249065\n",
      "cost at 3000 iteration is 0.4228431353432208\n",
      "cost at 3100 iteration is 0.42262578090532044\n",
      "cost at 3200 iteration is 0.4224163506180466\n",
      "cost at 3300 iteration is 0.42221418248223747\n",
      "cost at 3400 iteration is 0.4220186901637542\n",
      "cost at 3500 iteration is 0.42182935277298567\n",
      "cost at 3600 iteration is 0.42164570619560027\n",
      "cost at 3700 iteration is 0.42146733571705797\n",
      "cost at 3800 iteration is 0.4212938697294381\n",
      "cost at 3900 iteration is 0.421124974346345\n",
      "cost at 4000 iteration is 0.4209603487818472\n",
      "cost at 4100 iteration is 0.4207997213740161\n",
      "cost at 4200 iteration is 0.42064284615376507\n",
      "cost at 4300 iteration is 0.42048949987620765\n",
      "cost at 4400 iteration is 0.4203394794453518\n",
      "cost at 4500 iteration is 0.4201925996741633\n",
      "cost at 4600 iteration is 0.4200486913313148\n",
      "cost at 4700 iteration is 0.4199075994336317\n",
      "cost at 4800 iteration is 0.419769181749643\n",
      "cost at 4900 iteration is 0.4196333074849702\n",
      "cost at 5000 iteration is 0.4194998561247411\n",
      "cost at 5100 iteration is 0.41936871641193046\n",
      "cost at 5200 iteration is 0.41923978544365365\n",
      "cost at 5300 iteration is 0.4191129678700612\n",
      "cost at 5400 iteration is 0.41898817518269\n",
      "cost at 5500 iteration is 0.41886532508098934\n",
      "cost at 5600 iteration is 0.41874434090732193\n",
      "cost at 5700 iteration is 0.41862515114206933\n",
      "cost at 5800 iteration is 0.418507688951615\n",
      "cost at 5900 iteration is 0.4183918917829391\n",
      "cost at 6000 iteration is 0.41827770099938916\n",
      "cost at 6100 iteration is 0.41816506155289535\n",
      "cost at 6200 iteration is 0.41805392168850386\n",
      "cost at 6300 iteration is 0.41794423267762576\n",
      "cost at 6400 iteration is 0.4178359485768415\n",
      "cost at 6500 iteration is 0.417729026009493\n",
      "cost at 6600 iteration is 0.41762342396762814\n",
      "cost at 6700 iteration is 0.41751910363215045\n",
      "cost at 6800 iteration is 0.4174160282092815\n",
      "cost at 6900 iteration is 0.41731416278166233\n",
      "cost at 7000 iteration is 0.4172134741726095\n",
      "cost at 7100 iteration is 0.41711393082221093\n",
      "cost at 7200 iteration is 0.41701550267409204\n",
      "cost at 7300 iteration is 0.41691816107180957\n",
      "cost at 7400 iteration is 0.41682187866394727\n",
      "cost at 7500 iteration is 0.4167266293170827\n",
      "cost at 7600 iteration is 0.4166323880358839\n",
      "cost at 7700 iteration is 0.41653913088967365\n",
      "cost at 7800 iteration is 0.41644683494486445\n",
      "cost at 7900 iteration is 0.41635547820272995\n",
      "cost at 8000 iteration is 0.4162650395420338\n",
      "cost at 8100 iteration is 0.4161754986660808\n",
      "cost at 8200 iteration is 0.41608683605380203\n",
      "cost at 8300 iteration is 0.4159990329145223\n",
      "cost at 8400 iteration is 0.4159120711460908\n",
      "cost at 8500 iteration is 0.4158259332960885\n",
      "cost at 8600 iteration is 0.4157406025258515\n",
      "cost at 8700 iteration is 0.41565606257707494\n",
      "cost at 8800 iteration is 0.41557229774078286\n",
      "cost at 8900 iteration is 0.4154892928284704\n",
      "cost at 9000 iteration is 0.4154070331452411\n",
      "cost at 9100 iteration is 0.4153255044647785\n",
      "cost at 9200 iteration is 0.415244693006007\n",
      "cost at 9300 iteration is 0.4151645854113068\n",
      "cost at 9400 iteration is 0.4150851687261618\n",
      "cost at 9500 iteration is 0.4150064303801305\n",
      "cost at 9600 iteration is 0.41492835816903717\n",
      "cost at 9700 iteration is 0.41485094023829017\n",
      "cost at 9800 iteration is 0.41477416506724385\n",
      "cost at 9900 iteration is 0.41469802145452467\n",
      "--- 363.404s seconds ---\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "weights = train_logistic_regression(X_train_10k, y_train_10k, max_itr=10000, learning_rate=0.01, fit_intercept=True)\n",
    "print(\"--- %0.3fs seconds ---\" % (timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set is: 0.711\n"
     ]
    }
   ],
   "source": [
    "X_test_10k = X_test\n",
    "\n",
    "predictions = predict(X_test_10k, weights)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('The ROC AUC on testing set is: {0:.3f}'.format(roc_auc_score(y_test, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the score is better as compared to the one we recieved while using decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_sgd(X_train,y_train,weights,learning_rate):\n",
    "    \"\"\"\n",
    "    updates weights with stochastic gradient descent\n",
    "    ARGS - \n",
    "      X_trian,y_trian - ndarray - (features,labels)\n",
    "      weights - ndarray\n",
    "      learning_rate -float\n",
    "    Returns - \n",
    "      weights - updated weights\n",
    "    \"\"\"\n",
    "    for X_each,y_each in zip(X_train,y_train):\n",
    "        prediction = compute_prediction(X_each,weights)\n",
    "        weights_delta = X_each *(y_each - prediction)\n",
    "        weights += learning_rate*weights_delta\n",
    "    return weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, max_iter, learning_rate, fit_intercept=False):\n",
    "    \"\"\" Train a logistic regression model\n",
    "    Args:\n",
    "        X_train, y_train (numpy.ndarray, training data set)\n",
    "        max_iter (int, number of iterations)\n",
    "        learning_rate (float)\n",
    "        fit_intercept (bool, with an intercept w0 or not)\n",
    "    Returns:\n",
    "        numpy.ndarray, learned weights\n",
    "    \"\"\"\n",
    "    if fit_intercept:\n",
    "        intercept = np.ones((X_train.shape[0], 1))\n",
    "        X_train = np.hstack((intercept, X_train))\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "    for iteration in range(max_iter):\n",
    "        weights = update_weights_sgd(X_train, y_train, weights, learning_rate)\n",
    "        # Check the cost for every 2 (for example) iterations\n",
    "        if iteration % 2 == 0:\n",
    "            print(compute_cost(X_train, y_train, weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41496547913327814\n",
      "0.4060071128286624\n",
      "0.40104937451793143\n",
      "--- 1.169s seconds ---\n",
      "The ROC AUC on testing set is: 0.720\n"
     ]
    }
   ],
   "source": [
    "# Train the SGD model based on 10000 samples\n",
    "start_time = timeit.default_timer()\n",
    "weights = train_logistic_regression(X_train_10k, y_train_10k, max_iter=5, learning_rate=0.01, fit_intercept=True)\n",
    "print(\"--- %0.3fs seconds ---\" % (timeit.default_timer() - start_time))\n",
    "predictions = predict(X_test_10k, weights)\n",
    "print('The ROC AUC on testing set is: {0:.3f}'.format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "X_dict_train, y_train = read_ad_click_data(n)\n",
    "dict_one_hot_encoder = DictVectorizer(sparse=False)\n",
    "X_train = dict_one_hot_encoder.fit_transform(X_dict_train)\n",
    "\n",
    "X_train_100k = X_train\n",
    "y_train_100k = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SGD model based on 100000 samples\n",
    "start_time = timeit.default_timer()\n",
    "weights = train_logistic_regression(X_train_100k, y_train_100k, max_iter=5, learning_rate=0.01, fit_intercept=True)\n",
    "print(\"--- %0.3fs seconds ---\" % (timeit.default_timer() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
